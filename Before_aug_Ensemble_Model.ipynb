{"cells":[{"cell_type":"markdown","source":["#Importing Libraries"],"metadata":{"id":"SmcQZiSEPk-0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"x1e3YW2sENWb"},"outputs":[],"source":["from keras.callbacks import History\n","from keras.callbacks import ModelCheckpoint, TensorBoard\n","from keras.datasets import cifar10\n","from keras.engine import training\n","from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout, Activation, Average\n","from keras.losses import categorical_crossentropy\n","from keras.models import Model\n","from keras.optimizers import Adam\n","from keras.utils import to_categorical\n","from tensorflow.python.framework.ops import Tensor\n","from typing import Tuple, List\n","import glob\n","import re\n","import pandas as pd\n","import numpy as np\n","import os\n","\n","\n","\n","# Importing libraries\n","import os\n","import sys\n","import numpy as np\n","import keras\n","from keras.preprocessing.text import Tokenizer\n","from keras.utils import to_categorical\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Activation, Conv2D, Input, Embedding, Reshape, MaxPool2D, Concatenate, Flatten, Dropout, Dense, Conv1D\n","from keras.layers import MaxPool1D\n","from keras.models import Model\n","from keras.callbacks import ModelCheckpoint\n","from keras.optimizers import Adam\n","\n","import re\n","import pickle\n","import numpy as np\n","import pandas as pd\n","# Plot libraries\n","import seaborn as sns\n","#from wordcloud import WordCloud\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"S86LxP8SENWk"},"source":["Loading the Dataset"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KPHVhMh2Qbgd","executionInfo":{"status":"ok","timestamp":1669980689780,"user_tz":-60,"elapsed":25615,"user":{"displayName":"surbhi malik","userId":"07320802216105036180"}},"outputId":"36aeec98-287b-43fe-f660-337d4da3dfcc"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wBTOucctENWo"},"outputs":[],"source":["DATASET_ENCODING = \"ISO-8859-1\"\n","dataset = pd.read_csv('/content/drive/MyDrive/Thesis_Coding/Dataset1_PreAugmentation.csv', encoding=DATASET_ENCODING)\n","dataset = dataset[['Label','Text']]\n","dataset.head()"]},{"cell_type":"code","source":["DATASET_ENCODING = \"ISO-8859-1\"\n","dataset2 = pd.read_csv('/content/drive/MyDrive/Thesis_Coding/Dataset2_PreAugmentation.csv', encoding=DATASET_ENCODING)\n","dataset2 = dataset2[['Label','Text']]\n","dataset2.head()"],"metadata":{"id":"rR2p8oDfQyw_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s0PJhwpYENWp"},"source":["Ploting the Data --> Sentiment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bRP13hWOENWq"},"outputs":[],"source":["ax = dataset.groupby('Label').count().plot(kind='bar', title='Distribution of data', legend=True)\n","ax = ax.set_xticklabels(['0','1','2'], rotation=0)"]},{"cell_type":"code","source":["ax = dataset2.groupby('Label').count().plot(kind='bar', title='Distribution of data', legend=True)\n","ax = ax.set_xticklabels(['0','1','2'], rotation=0)"],"metadata":{"id":"ZOMSlBhMQ7bX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7cpzOjwcENWr"},"source":["Preprocessing the Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A3sfUww2ENWs"},"outputs":[],"source":["# Reading contractions.csv and storing it as a dict.\n","contractions = pd.read_csv('/content/drive/MyDrive/Thesis_Coding/preprocessing_contractions.csv', index_col='Contraction')\n","contractions.index = contractions.index.str.lower()\n","contractions.Meaning = contractions.Meaning.str.lower()\n","contractions_dict = contractions.to_dict()['Meaning']\n","\n","# Defining regex patterns.\n","urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|(www\\.)[^ ]*)\"\n","userPattern       = '@[^\\s]+'\n","hashtagPattern    = '#[^\\s]+'\n","alphaPattern      = \"[^a-z0-9<>]\"\n","sequencePattern   = r\"(.)\\1\\1+\"\n","seqReplacePattern = r\"\\1\\1\"\n","\n","# Defining regex for emojis\n","smileemoji        = r\"[8:=;]['`\\-]?[)d]+\"\n","sademoji          = r\"[8:=;]['`\\-]?\\(+\"\n","neutralemoji      = r\"[8:=;]['`\\-]?[\\/|l*]\"\n","lolemoji          = r\"[8:=;]['`\\-]?p+\"\n","\n","def preprocess_apply(tweet):\n","\n","    tweet = tweet.lower()\n","\n","    # Replace all URls with '<url>'\n","    tweet = re.sub(urlPattern,'<url>',tweet)\n","    # Replace @USERNAME to '<user>'.\n","    tweet = re.sub(userPattern,'<user>', tweet)\n","    \n","    # Replace 3 or more consecutive letters by 2 letter.\n","    tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n","\n","    # Replace all emojis.\n","    tweet = re.sub(r'<3', '<heart>', tweet)\n","    tweet = re.sub(smileemoji, '<smile>', tweet)\n","    tweet = re.sub(sademoji, '<sadface>', tweet)\n","    tweet = re.sub(neutralemoji, '<neutralface>', tweet)\n","    tweet = re.sub(lolemoji, '<lolface>', tweet)\n","\n","    for contraction, replacement in contractions_dict.items():\n","        tweet = tweet.replace(contraction, replacement)\n","\n","    # Remove non-alphanumeric and symbols\n","    tweet = re.sub(alphaPattern, ' ', tweet)\n","\n","    # Adding space on either side of '/' to seperate words (After replacing URLS).\n","    tweet = re.sub(r'/', ' / ', tweet)\n","    return tweet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UXOTatHHENWt"},"outputs":[],"source":["dataset['processed_text'] = dataset['Text'].apply(preprocess_apply)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sb8RUFaUENWv"},"outputs":[],"source":["dataset.head()"]},{"cell_type":"code","source":["dataset2['processed_text'] = dataset2['Text'].apply(preprocess_apply)"],"metadata":{"id":"HCxqhdHnSVwI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset2.head()"],"metadata":{"id":"CBDAGCUPSayX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xwwu5t9oENWz"},"source":["Showing the Preprocess the Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G64C158uENW0"},"outputs":[],"source":["count=0\n","for row in dataset.itertuples():\n","    print(\"Text:\", row[2])\n","    print(\"Processed:\", row[3])\n","    count+=1\n","    if count>10:\n","        break"]},{"cell_type":"code","source":["count=0\n","for row in dataset2.itertuples():\n","    print(\"Text:\", row[2])\n","    print(\"Processed:\", row[3])\n","    count+=1\n","    if count>10:\n","        break"],"metadata":{"id":"Ej5WPzPJSk4d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"giK5E0ruENW1"},"source":["Word2Vec"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"swti_-28ENW3"},"outputs":[],"source":["import spacy\n","!python -m spacy download en_core_web_lg\n","import en_core_web_lg\n","nlp = en_core_web_lg.load()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U80S_UrgENW4"},"outputs":[],"source":["x = 'king man woman'\n","doc =nlp(x)\n","for token1 in doc:\n","    for token2 in doc:\n","        print(token1.text, token2.text, token1.similarity(token2))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VV2FD5gTENW6"},"outputs":[],"source":["def get_vec(x):\n","    doc = nlp(x)\n","    vec = doc.vector\n","    return vec"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ehXTCkTLENW7"},"outputs":[],"source":["dataset['vec']= dataset['processed_text'].apply(lambda x: get_vec(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rICYsZWzENW9"},"outputs":[],"source":["dataset.head()"]},{"cell_type":"code","source":["dataset2['vec']= dataset2['processed_text'].apply(lambda x: get_vec(x))"],"metadata":{"id":"iuhx_ADuX00U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset2.head()"],"metadata":{"id":"bRkyMpI-X6qD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Sy0OMobENW-"},"source":["Spliting the dataset1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I0YgJ-XJENXB"},"outputs":[],"source":["from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZnyatSGPENXC"},"outputs":[],"source":["#X_data, Y_data = np.array(dataset['vec']), np.array(dataset['Label'])\n","X_data, Y_data = dataset['Text'], dataset['Label']\n","X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, test_size = 0.02, random_state = 0)\n","print('Data Split done.')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"soqO2bDOENXD"},"outputs":[],"source":["X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"]},{"cell_type":"markdown","source":["Spliting the dataset2"],"metadata":{"id":"8Lc0CenoaTD9"}},{"cell_type":"code","source":["X_data2, Y_data2 = dataset2['Text'], dataset2['Label']\n","X_train2, X_test2, Y_train2, Y_test2 = train_test_split(X_data2, Y_data2, test_size = 0.02, random_state = 0)\n","print('Data Split done.')"],"metadata":{"id":"cePQ_VLkaXhb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train2.shape, X_test2.shape, Y_train2.shape, Y_test2.shape"],"metadata":{"id":"FBg0DskmabM0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gV1Hp9RuENXE"},"source":["# CNN: Dataset1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DAF5JsVXENXF"},"outputs":[],"source":["\n","#####----- Before running this model run the train-test split portion once -----#####\n","\n","from keras.preprocessing.text import Tokenizer\n","tokenizer = Tokenizer(num_words=5000)\n","tokenizer.fit_on_texts(X_train)\n","Xcnn_train = tokenizer.texts_to_sequences(X_train)\n","Xcnn_test = tokenizer.texts_to_sequences(X_test)\n","vocab_size = len(tokenizer.word_index) + 1  \n","print(X_train[1])\n","print(Xcnn_train[1]) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sRDLRJhaENXG"},"outputs":[],"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","maxlen = 100\n","Xcnn_train = pad_sequences(Xcnn_train, padding='post', maxlen=maxlen)\n","Xcnn_test = pad_sequences(Xcnn_test, padding='post', maxlen=maxlen)\n","print(Xcnn_train[0, :]) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WHbPv5nkENXH"},"outputs":[],"source":["from keras.models import Sequential\n","from keras import layers \n","embedding_dim = 200\n","textcnnmodel = Sequential()\n","textcnnmodel.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n","textcnnmodel.add(layers.Conv1D(128, 5, activation='relu'))\n","textcnnmodel.add(layers.GlobalMaxPooling1D())\n","textcnnmodel.add(layers.Dense(10, activation='relu'))\n","textcnnmodel.add(layers.Dense(1, activation='sigmoid'))\n","textcnnmodel.compile(optimizer='adam',\n","               loss='binary_crossentropy',\n","               metrics=['accuracy'])\n","textcnnmodel.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IbfPSXnzENXJ"},"outputs":[],"source":["textcnnmodel.fit(Xcnn_train, Y_train,\n","                     epochs=100,\n","                     verbose=False,\n","                     validation_data=(Xcnn_test, Y_test),\n","                     batch_size=10)\n","loss, accuracy = textcnnmodel.evaluate(Xcnn_train, Y_train, verbose=False)\n","print(\"Training Accuracy: {:.4f}\".format(accuracy))\n","loss, accuracy = textcnnmodel.evaluate(Xcnn_test, Y_test, verbose=False)\n","print(\"Testing Accuracy:  {:.4f}\".format(accuracy)) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yfRqaUWgENXJ"},"outputs":[],"source":["textcnnmodel.save('/content/drive/MyDrive/Thesis_Coding/Dataset1_bef augu/saved_models/CNN.hdf5')"]},{"cell_type":"markdown","source":["#CNN: Dataset2"],"metadata":{"id":"p4YHti6-aosR"}},{"cell_type":"code","source":["from keras.preprocessing.text import Tokenizer\n","tokenizer = Tokenizer(num_words=5000)\n","tokenizer.fit_on_texts(X_train2)\n","Xcnn2_train = tokenizer.texts_to_sequences(X_train2)\n","Xcnn2_test = tokenizer.texts_to_sequences(X_test2)\n","vocab_size = len(tokenizer.word_index) + 1  \n","print(X_train2[1])\n","print(Xcnn2_train[1]) "],"metadata":{"id":"Gf07iiD3am2I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","maxlen = 100\n","Xcnn2_train = pad_sequences(Xcnn2_train, padding='post', maxlen=maxlen)\n","Xcnn2_test = pad_sequences(Xcnn2_test, padding='post', maxlen=maxlen)\n","print(Xcnn2_train[0, :]) "],"metadata":{"id":"NO-DqSL7bEUj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.models import Sequential\n","from keras import layers \n","embedding_dim = 200\n","textcnnmodel = Sequential()\n","textcnnmodel.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n","textcnnmodel.add(layers.Conv1D(128, 5, activation='relu'))\n","textcnnmodel.add(layers.GlobalMaxPooling1D())\n","textcnnmodel.add(layers.Dense(10, activation='relu'))\n","textcnnmodel.add(layers.Dense(1, activation='sigmoid'))\n","textcnnmodel.compile(optimizer='adam',\n","               loss='binary_crossentropy',\n","               metrics=['accuracy'])\n","textcnnmodel.summary()"],"metadata":{"id":"SBGD4j3jbFTP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["textcnnmodel.fit(Xcnn2_train, Y_train2,\n","                     epochs=100,\n","                     verbose=False,\n","                     validation_data=(Xcnn2_test, Y_test2),\n","                     batch_size=10)\n","loss, accuracy = textcnnmodel.evaluate(Xcnn2_train, Y_train2, verbose=False)\n","print(\"Training Accuracy: {:.4f}\".format(accuracy))\n","loss, accuracy = textcnnmodel.evaluate(Xcnn2_test, Y_test2, verbose=False)\n","print(\"Testing Accuracy:  {:.4f}\".format(accuracy)) "],"metadata":{"id":"Y-GPbvAabJaR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["textcnnmodel.save('/content/drive/MyDrive/Thesis_Coding/Dataset2_bef augu/saved_models/CNN.hdf5')"],"metadata":{"id":"H0VYyAkSbLVT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cQQ8LwCFENXK"},"source":["#BiLSTM: *Dataset1*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bSA0HDaiENXL"},"outputs":[],"source":["\n","#####----- Before running this model run the train-test split portion once -----#####\n","\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, Dense, LSTM, Conv1D, Embedding\n","input_length = 60\n","vocab_length = 60000\n","Embedding_dimensions = 100\n","\n","vocab_length = 60000\n","\n","tokenizer = Tokenizer(filters=\"\", lower=False, oov_token=\"<oov>\")\n","tokenizer.fit_on_texts(X_data)\n","tokenizer.num_words = vocab_length\n","print(\"Tokenizer vocab length:\", vocab_length)\n","\n","X_train = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=input_length)\n","X_test  = pad_sequences(tokenizer.texts_to_sequences(X_test) , maxlen=input_length)\n","\n","print(\"X_train.shape:\", X_train.shape)\n","print(\"X_test.shape :\", X_test.shape)\n","\n","embedding_matrix = np.zeros((vocab_length, Embedding_dimensions))\n","def getModel():\n","    embedding_layer = Embedding(input_dim = vocab_length,\n","                                output_dim = Embedding_dimensions,\n","                                weights=[embedding_matrix],\n","                                input_length=input_length,\n","                                trainable=False)\n","\n","    model = Sequential([\n","        embedding_layer,\n","        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),\n","        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),\n","        Conv1D(100, 5, activation='relu'),\n","        GlobalMaxPool1D(),\n","        Dense(16, activation='relu'),\n","        Dense(1, activation='sigmoid'),\n","    ],\n","    name=\"Sentiment_Model\")\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oMgRlhwxENXM"},"outputs":[],"source":["training_model = getModel()\n","training_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D9Ku59K_ENXN"},"outputs":[],"source":["training_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9m8UTRFfENXN"},"outputs":[],"source":["from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n","\n","callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n","             EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=5)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kSe8198mENXP"},"outputs":[],"source":["history = training_model.fit(\n","    X_train, Y_train,\n","    batch_size=1024,\n","    epochs=100,\n","    validation_split=0.1,\n","    callbacks=callbacks,\n","    verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_loBmt25ENXS"},"outputs":[],"source":["training_model.save('/content/drive/MyDrive/Thesis_Coding/Dataset1_bef augu/saved_models/BiLSTM.hdf5')"]},{"cell_type":"markdown","source":["\n","#BiLSTM: *Dataset2*\n"],"metadata":{"id":"jnk7fIzEd6Pe"}},{"cell_type":"code","source":["#####----- Before running this model run the train-test split portion once -----#####\n","\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, Dense, LSTM, Conv1D, Embedding\n","input_length = 60\n","vocab_length = 60000\n","Embedding_dimensions = 100\n","\n","vocab_length = 60000\n","\n","tokenizer = Tokenizer(filters=\"\", lower=False, oov_token=\"<oov>\")\n","tokenizer.fit_on_texts(X_data2)\n","tokenizer.num_words = vocab_length\n","print(\"Tokenizer vocab length:\", vocab_length)\n","\n","X_train2 = pad_sequences(tokenizer.texts_to_sequences(X_train2), maxlen=input_length)\n","X_test2  = pad_sequences(tokenizer.texts_to_sequences(X_test2) , maxlen=input_length)\n","\n","print(\"X_train2.shape:\", X_train2.shape)\n","print(\"X_test2.shape :\", X_test2.shape)\n","\n","embedding_matrix = np.zeros((vocab_length, Embedding_dimensions))\n","def getModel():\n","    embedding_layer = Embedding(input_dim = vocab_length,\n","                                output_dim = Embedding_dimensions,\n","                                weights=[embedding_matrix],\n","                                input_length=input_length,\n","                                trainable=False)\n","\n","    model = Sequential([\n","        embedding_layer,\n","        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),\n","        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),\n","        Conv1D(100, 5, activation='relu'),\n","        GlobalMaxPool1D(),\n","        Dense(16, activation='relu'),\n","        Dense(1, activation='sigmoid'),\n","    ],\n","    name=\"Sentiment_Model\")\n","    return model"],"metadata":{"id":"tTzkANLXd53a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_model = getModel()\n","training_model.summary()"],"metadata":{"id":"ZBwrpyL9eFLG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"],"metadata":{"id":"9v_Q71c5eHp4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n","\n","callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n","             EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=5)]"],"metadata":{"id":"oclv_NYleQwX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = training_model.fit(\n","    X_train2, Y_train2,\n","    batch_size=1024,\n","    epochs=100,\n","    validation_split=0.1,\n","    callbacks=callbacks,\n","    verbose=1,\n",")"],"metadata":{"id":"3ptMrN3jeTZI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_model.save('/content/drive/MyDrive/Thesis_Coding/Dataset2_bef augu/saved_models/BiLSTM.hdf5')"],"metadata":{"id":"N3d85mzjeUO9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0EO4b0GgENXX"},"source":["# LSTM: Dataset1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jO187vE2ENXa"},"outputs":[],"source":["\n","#####----- Before running this model run the train-test split portion once -----#####\n","\n","# Define parameter\n","max_len = 50 \n","trunc_type = 'post'\n","padding_type = 'post'\n","oov_tok = '<OOV>' # out of vocabulary token\n","vocab_size = 500\n","embedding_dim = 100\n","tokenizer = Tokenizer(num_words = vocab_size, \n","                      char_level = False,\n","                      oov_token = oov_tok)\n","tokenizer.fit_on_texts(X_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sSSLtNQcENXc"},"outputs":[],"source":["# Get the word_index\n","word_index = tokenizer.word_index\n","total_words = len(word_index)\n","total_words"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kv9Bgvj5ENXd"},"outputs":[],"source":["training_sequences = tokenizer.texts_to_sequences(X_train)\n","training_padded = pad_sequences(training_sequences,\n","                                maxlen = max_len,\n","                                padding = padding_type,\n","                                truncating = trunc_type)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4UpLgU4GENXf"},"outputs":[],"source":["testing_sequences = tokenizer.texts_to_sequences(X_test)\n","testing_padded = pad_sequences(testing_sequences,\n","                               maxlen = max_len,\n","                               padding = padding_type,\n","                               truncating = trunc_type)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RH7eju9MENXg"},"outputs":[],"source":["n_lstm = 128\n","drop_lstm = 0.2\n","# Define LSTM Model \n","model1 = Sequential()\n","model1.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n","model1.add(tf.keras.layers.SpatialDropout1D(drop_lstm))\n","model1.add(LSTM(n_lstm, return_sequences=False))\n","model1.add(Dropout(drop_lstm))\n","model1.add(Dense(1, activation='sigmoid'))\n","model1.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4HRLxxZEENXh"},"outputs":[],"source":["model1.compile(loss = 'binary_crossentropy',\n","               optimizer = 'adam',\n","               metrics = ['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5GScdYJDENXk"},"outputs":[],"source":["epochs = 100\n","early_stop = EarlyStopping(monitor='val_loss', patience=2)\n","history = model1.fit(training_padded,\n","                     Y_train,\n","                     epochs=epochs, \n","                     validation_data=(testing_padded, Y_test),\n","                     callbacks =[early_stop],\n","                     verbose=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JVvcrZYTENXm"},"outputs":[],"source":["model1.save('/content/drive/MyDrive/Thesis_Coding/Dataset1_bef augu/saved_models/LSTM.hdf5')"]},{"cell_type":"markdown","source":["# LSTM: Dataset2\n"],"metadata":{"id":"YisM93wjmEhk"}},{"cell_type":"code","source":["\n","#####----- Before running this model run the train-test split portion once -----#####\n","\n","# Define parameter\n","max_len = 50 \n","trunc_type = 'post'\n","padding_type = 'post'\n","oov_tok = '<OOV>' # out of vocabulary token\n","vocab_size = 500\n","embedding_dim = 100\n","tokenizer = Tokenizer(num_words = vocab_size, \n","                      char_level = False,\n","                      oov_token = oov_tok)\n","tokenizer.fit_on_texts(X_train)"],"metadata":{"id":"6x-79PUEmHAh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the word_index\n","word_index = tokenizer.word_index\n","total_words = len(word_index)\n","total_words"],"metadata":{"id":"GLlytqtdmJOi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_sequences2 = tokenizer.texts_to_sequences(X_train2)\n","training_padded2 = pad_sequences(training_sequences2,\n","                                maxlen = max_len,\n","                                padding = padding_type,\n","                                truncating = trunc_type)"],"metadata":{"id":"m_bAlgJwmNC2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["testing_sequences2 = tokenizer.texts_to_sequences(X_test2)\n","testing_padded2 = pad_sequences(testing_sequences2,\n","                               maxlen = max_len,\n","                               padding = padding_type,\n","                               truncating = trunc_type)"],"metadata":{"id":"aTXkDD4NmNsk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n_lstm = 128\n","drop_lstm = 0.2\n","# Define LSTM Model \n","model1 = Sequential()\n","model1.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n","model1.add(tf.keras.layers.SpatialDropout1D(drop_lstm))\n","model1.add(LSTM(n_lstm, return_sequences=False))\n","model1.add(Dropout(drop_lstm))\n","model1.add(Dense(1, activation='sigmoid'))\n","model1.summary()"],"metadata":{"id":"fzylX3T4mQSH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model1.compile(loss = 'binary_crossentropy',\n","               optimizer = 'adam',\n","               metrics = ['accuracy'])"],"metadata":{"id":"TplV0LJamSgd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = 100\n","early_stop = EarlyStopping(monitor='val_loss', patience=2)\n","history = model1.fit(training_padded2,\n","                     Y_train2,\n","                     epochs=epochs, \n","                     validation_data=(testing_padded2, Y_test2),\n","                     callbacks =[early_stop],\n","                     verbose=2)"],"metadata":{"id":"HgGSWDxAmUkS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model1.save('/content/drive/MyDrive/Thesis_Coding/Dataset2_bef augu/saved_models/LSTM.hdf5')"],"metadata":{"id":"1O5J1xlRmWdp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"45B9cuKNENXo"},"source":["#Ensemble Model: Dataset1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tPMMZrQmENXp"},"outputs":[],"source":["from keras.models import load_model\n","from sklearn.metrics import accuracy_score\n","\n","model1 = load_model('/content/drive/MyDrive/Thesis_Coding/Dataset1_bef augu/saved_models/CNN.hdf5')\n","model2 = load_model('/content/drive/MyDrive/Thesis_Coding/Dataset1_bef augu/saved_models/LSTM.hdf5')\n","model3 = load_model('/content/drive/MyDrive/Thesis_Coding/Dataset1_bef augu/saved_models/BiLSTM.hdf5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YZRRJOxWENXq"},"outputs":[],"source":["models = [model1, model2, model3]\n","\n","preds = [model.predict(X_test) for model in models]\n","preds=np.array(preds)\n","summed = np.sum(preds, axis=0)\n","\n","# argmax across classes\n","ensemble_prediction = np.argmax(summed, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lry_cr8JENXr"},"outputs":[],"source":["prediction1 = model1.predict(X_test) \n","prediction2 = model2.predict(X_test) \n","prediction3 = model3.predict(X_test) \n","\n","accuracy1 = accuracy_score(Y_test, prediction1)\n","accuracy2 = accuracy_score(Y_test, prediction2)\n","accuracy3 = accuracy_score(Y_test, prediction3)\n","ensemble_accuracy = accuracy_score(Y_test, ensemble_prediction)\n","\n","print('Accuracy Score for model1 = ', accuracy1)\n","print('Accuracy Score for model2 = ', accuracy2)\n","print('Accuracy Score for model3 = ', accuracy3)\n","print('Accuracy Score for average ensemble = ', ensemble_accuracy)\n","\n","from sklearn.metrics import classification_report\n","# Print the evaluation metrics for the dataset.\n","# print(classification_report(Y_test, preds))\n","print(classification_report(Y_test, ensemble_accuracy))"]},{"cell_type":"markdown","source":["# Ensemble model: dataset 2\n"],"metadata":{"id":"rScublu0o8YM"}},{"cell_type":"code","source":["from keras.models import load_model\n","from sklearn.metrics import accuracy_score\n","\n","model1 = load_model('/content/drive/MyDrive/Thesis_Coding/Dataset2_bef augu/saved_models/CNN.hdf5')\n","model2 = load_model('/content/drive/MyDrive/Thesis_Coding/Dataset2_bef augu/saved_models/LSTM.hdf5')\n","model3 = load_model('/content/drive/MyDrive/Thesis_Coding/Dataset2_bef augu/saved_models/BiLSTM.hdf5')"],"metadata":{"id":"Mw7xGdzLpCKg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models = [model1, model2, model3]\n","\n","preds = [model.predict(X_test2) for model in models]\n","preds=np.array(preds)\n","summed = np.sum(preds, axis=0)\n","\n","# argmax across classes\n","ensemble_prediction = np.argmax(summed, axis=1)"],"metadata":{"id":"-chZ_D5apMSV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prediction1 = model1.predict(X_test2) \n","prediction2 = model2.predict(X_test2) \n","prediction3 = model3.predict(X_test2) \n","\n","accuracy1 = accuracy_score(Y_test2, prediction1)\n","accuracy2 = accuracy_score(Y_test2, prediction2)\n","accuracy3 = accuracy_score(Y_test2, prediction3)\n","ensemble_accuracy = accuracy_score(Y_test2, ensemble_prediction)\n","\n","print('Accuracy Score for model1 = ', accuracy1)\n","print('Accuracy Score for model2 = ', accuracy2)\n","print('Accuracy Score for model3 = ', accuracy3)\n","print('Accuracy Score for average ensemble = ', ensemble_accuracy)\n","\n","from sklearn.metrics import classification_report\n","# Print the evaluation metrics for the dataset.\n","# print(classification_report(Y_test, preds))\n","print(classification_report(Y_test2, ensemble_accuracy))"],"metadata":{"id":"TDuJODFTpOlt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nI6a6AhPENXt"},"source":["# BiLSTM Ensemble Model: Dataset1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AHfgZIOOENXu"},"outputs":[],"source":["\n","#####----- Before running this model run the train-test split portion once -----#####\n","\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, Dense, LSTM, Conv1D, Embedding\n","input_length = 200\n","vocab_length = 60000\n","Embedding_dimensions = 100\n","\n","tokenizer = Tokenizer(filters=\"\", lower=False, oov_token=\"<oov>\")\n","tokenizer.fit_on_texts(X_data)\n","tokenizer.num_words = vocab_length\n","print(\"Tokenizer vocab length:\", vocab_length)\n","\n","X_train = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=input_length)\n","X_test  = pad_sequences(tokenizer.texts_to_sequences(X_test) , maxlen=input_length)\n","\n","print(\"X_train.shape:\", X_train.shape)\n","print(\"X_test.shape :\", X_test.shape)\n","\n","embedding_matrix = np.zeros((vocab_length, Embedding_dimensions))\n","def getModel():\n","    embedding_layer = Embedding(input_dim = vocab_length,\n","                                output_dim = Embedding_dimensions,\n","                                weights=[embedding_matrix],\n","                                input_length=input_length,\n","                                trainable=False)\n","\n","    model = Sequential([\n","        embedding_layer,\n","        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),\n","        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),\n","        Conv1D(100, 5, activation='relu'),\n","        GlobalMaxPool1D(),\n","        Dense(16, activation='relu'),\n","        Dense(1, activation='sigmoid'),\n","    ],\n","    name=\"Sentiment_Model\")\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qi_UFlgCENXv"},"outputs":[],"source":["training_model = getModel()\n","training_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2_us0FXoENXv"},"outputs":[],"source":["training_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n","callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n","             EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=5)]\n","\n","history = training_model.fit(\n","    X_train, Y_train,\n","    batch_size=1024,\n","    epochs=100,\n","    validation_split=0.1,\n","    callbacks=callbacks,\n","    verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D7jcXaNKENXw"},"outputs":[],"source":["training_model.save('/content/drive/MyDrive/Thesis_Coding/Dataset1_bef augu/BiLSTM i1,i2,i3/BiLSTM i1.hdf5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wYGrHxyUENXx"},"outputs":[],"source":["\n","#####----- Before running this model run the train-test split portion once -----#####\n","\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, Dense, LSTM, Conv1D, Embedding\n","input_length = 600\n","vocab_length = 60000\n","Embedding_dimensions = 100\n","\n","tokenizer = Tokenizer(filters=\"\", lower=False, oov_token=\"<oov>\")\n","tokenizer.fit_on_texts(X_data)\n","tokenizer.num_words = vocab_length\n","print(\"Tokenizer vocab length:\", vocab_length)\n","\n","X_train = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=input_length)\n","X_test  = pad_sequences(tokenizer.texts_to_sequences(X_test) , maxlen=input_length)\n","\n","print(\"X_train.shape:\", X_train.shape)\n","print(\"X_test.shape :\", X_test.shape)\n","\n","embedding_matrix = np.zeros((vocab_length, Embedding_dimensions))\n","def getModel():\n","    embedding_layer = Embedding(input_dim = vocab_length,\n","                                output_dim = Embedding_dimensions,\n","                                weights=[embedding_matrix],\n","                                input_length=input_length,\n","                                trainable=False)\n","\n","    model = Sequential([\n","        embedding_layer,\n","        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),\n","        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),\n","        Conv1D(100, 5, activation='relu'),\n","        GlobalMaxPool1D(),\n","        Dense(16, activation='relu'),\n","        Dense(1, activation='sigmoid'),\n","    ],\n","    name=\"Sentiment_Model\")\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vuUi89QPENX0"},"outputs":[],"source":["training_model = getModel()\n","training_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eScVFfvBENX1"},"outputs":[],"source":["training_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n","callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n","             EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=5)]\n","\n","history = training_model.fit(\n","    X_train, Y_train,\n","    batch_size=1024,\n","    epochs=100,\n","    validation_split=0.1,\n","    callbacks=callbacks,\n","    verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DYE-3UxoENX3"},"outputs":[],"source":["training_model.save('/content/drive/MyDrive/Thesis_Coding/Dataset1_bef augu/BiLSTM i1,i2,i3/BiLSTM i2.hdf5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LUQi7qcCENX4"},"outputs":[],"source":["\n","#####----- Before running this model run the train-test split portion once -----#####\n","\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, Dense, LSTM, Conv1D, Embedding\n","input_length = 60\n","vocab_length = 60000\n","Embedding_dimensions = 100\n","\n","tokenizer = Tokenizer(filters=\"\", lower=False, oov_token=\"<oov>\")\n","tokenizer.fit_on_texts(X_data)\n","tokenizer.num_words = vocab_length\n","print(\"Tokenizer vocab length:\", vocab_length)\n","\n","X_train = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=input_length)\n","X_test  = pad_sequences(tokenizer.texts_to_sequences(X_test) , maxlen=input_length)\n","\n","print(\"X_train.shape:\", X_train.shape)\n","print(\"X_test.shape :\", X_test.shape)\n","\n","embedding_matrix = np.zeros((vocab_length, Embedding_dimensions))\n","def getModel():\n","    embedding_layer = Embedding(input_dim = vocab_length,\n","                                output_dim = Embedding_dimensions,\n","                                weights=[embedding_matrix],\n","                                input_length=input_length,\n","                                trainable=False)\n","\n","    model = Sequential([\n","        embedding_layer,\n","        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),\n","        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),\n","        Conv1D(100, 5, activation='relu'),\n","        GlobalMaxPool1D(),\n","        Dense(16, activation='relu'),\n","        Dense(1, activation='sigmoid'),\n","    ],\n","    name=\"Sentiment_Model\")\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lT7ADh26ENX5"},"outputs":[],"source":["training_model = getModel()\n","training_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sL1u4u_xENX6"},"outputs":[],"source":["training_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c4YaVl9vENX6"},"outputs":[],"source":["from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n","callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n","             EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=5)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ObyloTv6ENX7"},"outputs":[],"source":["history = training_model.fit(\n","    X_train, Y_train,\n","    batch_size=1024,\n","    epochs=100,\n","    validation_split=0.1,\n","    callbacks=callbacks,\n","    verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K9w5HRT4ENX7"},"outputs":[],"source":["training_model.save('/content/drive/MyDrive/Thesis_Coding/Dataset1_bef augu/BiLSTM i1,i2,i3/BilSTM i3.hdf5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PILfXeL6ENX8"},"outputs":[],"source":["from keras.models import load_model\n","from sklearn.metrics import accuracy_score\n","\n","model1 = load_model('/content/drive/MyDrive/Thesis_Coding/Dataset1_bef augu/BiLSTM i1,i2,i3/BiLSTM i1.hdf5')\n","model2 = load_model('/content/drive/MyDrive/Thesis_Coding/Dataset1_bef augu/BiLSTM i1,i2,i3/BiLSTM i2.hdf5')\n","model3 = load_model('/content/drive/MyDrive/Thesis_Coding/Dataset1_bef augu/BiLSTM i1,i2,i3/BilSTM i3.hdf5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IZwfN9npENX9"},"outputs":[],"source":["models = [model1, model2, model3]\n","\n","preds = [model.predict(X_test) for model in models]\n","preds=np.array(preds)\n","summed = np.sum(preds, axis=0)\n","\n","# argmax across classes\n","ensemble_prediction = np.argmax(summed, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iQbzo7aIENX9"},"outputs":[],"source":["prediction1 = model1.predict(X_test) \n","prediction2 = model2.predict(X_test) \n","prediction3 = model3.predict(X_test) \n","\n","accuracy1 = accuracy_score(Y_test, prediction1)\n","accuracy2 = accuracy_score(Y_test, prediction2)\n","accuracy3 = accuracy_score(Y_test, prediction3)\n","ensemble_accuracy = accuracy_score(Y_test, ensemble_prediction)\n","\n","print('Accuracy Score for model1 = ', accuracy1)\n","print('Accuracy Score for model2 = ', accuracy2)\n","print('Accuracy Score for model3 = ', accuracy3)\n","print('Accuracy Score for average ensemble = ', ensemble_accuracy)\n","\n","from sklearn.metrics import classification_report\n","# Print the evaluation metrics for the dataset.\n","# print(classification_report(Y_test, preds))\n","print(classification_report(Y_test, ensemble_accuracy))"]},{"cell_type":"markdown","source":["# BiLSTM Ensemble Model: Dataset2\n"],"metadata":{"id":"TM4xX-A0naPL"}},{"cell_type":"code","source":["#####----- Before running this model run the train-test split portion once -----#####\n","\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, Dense, LSTM, Conv1D, Embedding\n","input_length = 200\n","vocab_length = 60000\n","Embedding_dimensions = 100\n","\n","tokenizer = Tokenizer(filters=\"\", lower=False, oov_token=\"<oov>\")\n","tokenizer.fit_on_texts(X_data2)\n","tokenizer.num_words = vocab_length\n","print(\"Tokenizer vocab length:\", vocab_length)\n","\n","X_train2 = pad_sequences(tokenizer.texts_to_sequences(X_train2), maxlen=input_length)\n","X_test2  = pad_sequences(tokenizer.texts_to_sequences(X_test2) , maxlen=input_length)\n","\n","print(\"X_train2.shape:\", X_train2.shape)\n","print(\"X_test2.shape :\", X_test2.shape)\n","\n","embedding_matrix = np.zeros((vocab_length, Embedding_dimensions))\n","def getModel():\n","    embedding_layer = Embedding(input_dim = vocab_length,\n","                                output_dim = Embedding_dimensions,\n","                                weights=[embedding_matrix],\n","                                input_length=input_length,\n","                                trainable=False)\n","\n","    model = Sequential([\n","        embedding_layer,\n","        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),\n","        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),\n","        Conv1D(100, 5, activation='relu'),\n","        GlobalMaxPool1D(),\n","        Dense(16, activation='relu'),\n","        Dense(1, activation='sigmoid'),\n","    ],\n","    name=\"Sentiment_Model\")\n","    return model"],"metadata":{"id":"Ko4WQpbhnZqU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_modeli1 = getModel()\n","training_modeli1.summary()"],"metadata":{"id":"DYLjOQzRoD_9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n","callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n","             EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=5)]\n","\n","history = training_model2.fit(\n","    X_train2, Y_train2,\n","    batch_size=1024,\n","    epochs=100,\n","    validation_split=0.1,\n","    callbacks=callbacks,\n","    verbose=1,\n",")"],"metadata":{"id":"3R2YmFWAoMQL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_modeli1.save('/content/drive/MyDrive/Thesis_Coding/Dataset2_bef augu/BiLSTM i1,i2,i3/BiLSTM i1.hdf5')"],"metadata":{"id":"epsUlP5OoWuM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#####----- Before running this model run the train-test split portion once -----#####\n","\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, Dense, LSTM, Conv1D, Embedding\n","input_length = 600\n","vocab_length = 60000\n","Embedding_dimensions = 100\n","\n","tokenizer = Tokenizer(filters=\"\", lower=False, oov_token=\"<oov>\")\n","tokenizer.fit_on_texts(X_data2)\n","tokenizer.num_words = vocab_length\n","print(\"Tokenizer vocab length:\", vocab_length)\n","\n","X_train2 = pad_sequences(tokenizer.texts_to_sequences(X_train2), maxlen=input_length)\n","X_test2  = pad_sequences(tokenizer.texts_to_sequences(X_test2) , maxlen=input_length)\n","\n","print(\"X_train.shape:\", X_train2.shape)\n","print(\"X_test.shape :\", X_test2.shape)\n","\n","embedding_matrix = np.zeros((vocab_length, Embedding_dimensions))\n","def getModel():\n","    embedding_layer = Embedding(input_dim = vocab_length,\n","                                output_dim = Embedding_dimensions,\n","                                weights=[embedding_matrix],\n","                                input_length=input_length,\n","                                trainable=False)\n","\n","    model = Sequential([\n","        embedding_layer,\n","        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),\n","        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),\n","        Conv1D(100, 5, activation='relu'),\n","        GlobalMaxPool1D(),\n","        Dense(16, activation='relu'),\n","        Dense(1, activation='sigmoid'),\n","    ],\n","    name=\"Sentiment_Model\")\n","    return model"],"metadata":{"id":"QpWGjTfsoZKS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_modeli2 = getModel()\n","training_modeli2.summary()"],"metadata":{"id":"b6VlH7p7oh5K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n","callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n","             EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=5)]\n","\n","history = training_model.fit(\n","    X_train2, Y_train2,\n","    batch_size=1024,\n","    epochs=100,\n","    validation_split=0.1,\n","    callbacks=callbacks,\n","    verbose=1,\n",")"],"metadata":{"id":"_vIqY2moomJd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_modeli2.save('/content/drive/MyDrive/Thesis_Coding/Dataset2_bef augu/BiLSTM i1,i2,i3/BiLSTM i2.hdf5')"],"metadata":{"id":"WsRQeqg1owwu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#####----- Before running this model run the train-test split portion once -----#####\n","\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, Dense, LSTM, Conv1D, Embedding\n","input_length = 60\n","vocab_length = 60000\n","Embedding_dimensions = 100\n","\n","tokenizer = Tokenizer(filters=\"\", lower=False, oov_token=\"<oov>\")\n","tokenizer.fit_on_texts(X_data2)\n","tokenizer.num_words = vocab_length\n","print(\"Tokenizer vocab length:\", vocab_length)\n","\n","X_train2 = pad_sequences(tokenizer.texts_to_sequences(X_train2), maxlen=input_length)\n","X_test2  = pad_sequences(tokenizer.texts_to_sequences(X_test2) , maxlen=input_length)\n","\n","print(\"X_train.shape:\", X_train2.shape)\n","print(\"X_test.shape :\", X_test2.shape)\n","\n","embedding_matrix = np.zeros((vocab_length, Embedding_dimensions))\n","def getModel():\n","    embedding_layer = Embedding(input_dim = vocab_length,\n","                                output_dim = Embedding_dimensions,\n","                                weights=[embedding_matrix],\n","                                input_length=input_length,\n","                                trainable=False)\n","\n","    model = Sequential([\n","        embedding_layer,\n","        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),\n","        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),\n","        Conv1D(100, 5, activation='relu'),\n","        GlobalMaxPool1D(),\n","        Dense(16, activation='relu'),\n","        Dense(1, activation='sigmoid'),\n","    ],\n","    name=\"Sentiment_Model\")\n","    return model"],"metadata":{"id":"raCXhxqbowzh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_modeli3 = getModel()\n","training_modeli3.summary()"],"metadata":{"id":"ulH6uXaIow26"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_modeli3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"],"metadata":{"id":"3nziO2Uxow5C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n","callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n","             EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=5)]"],"metadata":{"id":"plddZATFpG1n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = training_modeli3.fit(\n","    X_train2, Y_train2,\n","    batch_size=1024,\n","    epochs=100,\n","    validation_split=0.1,\n","    callbacks=callbacks,\n","    verbose=1,\n",")"],"metadata":{"id":"6uhBw0AkpKJb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training_modeli3.save('/content/drive/MyDrive/Thesis_Coding/Dataset2_bef augu/BiLSTM i1,i2,i3/BilSTM i3.hdf5')"],"metadata":{"id":"7NOaDg1xpR8T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.models import load_model\n","from sklearn.metrics import accuracy_score\n","\n","model1 = load_model('/content/drive/MyDrive/Thesis_Coding/Dataset2_bef augu/BiLSTM i1,i2,i3/BiLSTM i1.hdf5')\n","model2 = load_model('/content/drive/MyDrive/Thesis_Coding/Dataset2_bef augu/BiLSTM i1,i2,i3/BiLSTM i2.hdf5')\n","model3 = load_model('/content/drive/MyDrive/Thesis_Coding/Dataset2_bef augu/BiLSTM i1,i2,i3/BilSTM i3.hdf5')"],"metadata":{"id":"uS5qc6EmpR-v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["models = [model1, model2, model3]\n","\n","preds = [model.predict(X_test) for model in models]\n","preds=np.array(preds)\n","summed = np.sum(preds, axis=0)\n","\n","# argmax across classes\n","ensemble_prediction = np.argmax(summed, axis=1)"],"metadata":{"id":"hwvbHqnepSBN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prediction1 = model1.predict(X_test2) \n","prediction2 = model2.predict(X_test2) \n","prediction3 = model3.predict(X_test2) \n","\n","accuracy1 = accuracy_score(Y_test2, prediction1)\n","accuracy2 = accuracy_score(Y_test2, prediction2)\n","accuracy3 = accuracy_score(Y_test2, prediction3)\n","ensemble_accuracy = accuracy_score(Y_test, ensemble_prediction)\n","\n","print('Accuracy Score for model1 = ', accuracy1)\n","print('Accuracy Score for model2 = ', accuracy2)\n","print('Accuracy Score for model3 = ', accuracy3)\n","print('Accuracy Score for average ensemble = ', ensemble_accuracy)\n","\n","from sklearn.metrics import classification_report\n","# Print the evaluation metrics for the dataset.\n","# print(classification_report(Y_test, preds))\n","print(classification_report(Y_test2, ensemble_accuracy))"],"metadata":{"id":"Dw9xC5WbpSDY"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.2 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"598a2f3b50c1c2dda71daf7aff18104181475cf9b132f21d46967a184a024fbc"}},"colab":{"provenance":[],"collapsed_sections":["SmcQZiSEPk-0","gV1Hp9RuENXE","p4YHti6-aosR","cQQ8LwCFENXK","jnk7fIzEd6Pe","0EO4b0GgENXX","YisM93wjmEhk","45B9cuKNENXo","rScublu0o8YM","nI6a6AhPENXt","TM4xX-A0naPL"]}},"nbformat":4,"nbformat_minor":0}