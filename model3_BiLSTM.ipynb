{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"6X7TtOCjYWEO"},"outputs":[],"source":["import re\n","import pickle\n","import numpy as np\n","import pandas as pd\n","# Plot libraries\n","import seaborn as sns\n","#from wordcloud import WordCloud\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S9BN4cdgYWEQ"},"outputs":[],"source":["# Importing the dataset\n","#DATASET_COLUMNS  = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n","DATASET_ENCODING = \"ISO-8859-1\"\n","dataset = pd.read_csv('/content/drive/MyDrive/Thesis Datasets/NLP/Dataset1.csv', encoding=DATASET_ENCODING)\n","dataset.head()"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"2eHppUojZByk"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0X7GHwSKYWES"},"outputs":[],"source":["# Removing the unnecessary columns.\n","dataset = dataset[['Label','Text']]\n","\n","# Replacing the values.\n","dataset['sentiment'] = dataset['Label'].replace(4,1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gpSdlfELYWET"},"outputs":[],"source":["ax = dataset.groupby('sentiment').count().plot(kind='bar', title='Distribution of data', legend=True)\n","ax = ax.set_xticklabels(['0','1','2'], rotation=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c8rRNbDCYWEU"},"outputs":[],"source":["# Reading contractions.csv and storing it as a dict.\n","contractions = pd.read_csv('/content/drive/MyDrive/Thesis Datasets/NLP/contractions.csv', index_col='Contraction')\n","contractions.index = contractions.index.str.lower()\n","contractions.Meaning = contractions.Meaning.str.lower()\n","contractions_dict = contractions.to_dict()['Meaning']\n","\n","# Defining regex patterns.\n","urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|(www\\.)[^ ]*)\"\n","userPattern       = '@[^\\s]+'\n","hashtagPattern    = '#[^\\s]+'\n","alphaPattern      = \"[^a-z0-9<>]\"\n","sequencePattern   = r\"(.)\\1\\1+\"\n","seqReplacePattern = r\"\\1\\1\"\n","\n","# Defining regex for emojis\n","smileemoji        = r\"[8:=;]['`\\-]?[)d]+\"\n","sademoji          = r\"[8:=;]['`\\-]?\\(+\"\n","neutralemoji      = r\"[8:=;]['`\\-]?[\\/|l*]\"\n","lolemoji          = r\"[8:=;]['`\\-]?p+\"\n","\n","def preprocess_apply(tweet):\n","\n","    tweet = tweet.lower()\n","\n","    # Replace all URls with '<url>'\n","    tweet = re.sub(urlPattern,'<url>',tweet)\n","    # Replace @USERNAME to '<user>'.\n","    tweet = re.sub(userPattern,'<user>', tweet)\n","    \n","    # Replace 3 or more consecutive letters by 2 letter.\n","    tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n","\n","    # Replace all emojis.\n","    tweet = re.sub(r'<3', '<heart>', tweet)\n","    tweet = re.sub(smileemoji, '<smile>', tweet)\n","    tweet = re.sub(sademoji, '<sadface>', tweet)\n","    tweet = re.sub(neutralemoji, '<neutralface>', tweet)\n","    tweet = re.sub(lolemoji, '<lolface>', tweet)\n","\n","    for contraction, replacement in contractions_dict.items():\n","        tweet = tweet.replace(contraction, replacement)\n","\n","    # Remove non-alphanumeric and symbols\n","    tweet = re.sub(alphaPattern, ' ', tweet)\n","\n","    # Adding space on either side of '/' to seperate words (After replacing URLS).\n","    tweet = re.sub(r'/', ' / ', tweet)\n","    return tweet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rAJTfou2YWEV"},"outputs":[],"source":["dataset['processed_text'] = dataset['Text'].apply(preprocess_apply)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tO8XC11uYWEW"},"outputs":[],"source":["dataset.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i96M4vjKYWEX"},"outputs":[],"source":["count=0\n","for row in dataset.itertuples():\n","    print(\"Text:\", row[2])\n","    print(\"Processed:\", row[3])\n","    count+=1\n","    if count>10:\n","        break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PxjnVfOJYWEX"},"outputs":[],"source":["from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6FFFmD4sYWEY"},"outputs":[],"source":["X_data, y_data = np.array(dataset['processed_text']), np.array(dataset['sentiment'])\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.05, random_state = 0)\n","print('Data Split done.')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D3vGXcCnYWEZ"},"outputs":[],"source":["from gensim.models import Word2Vec\n","\n","Embedding_dimensions = 100\n","\n","# Creating Word2Vec training dataset.\n","Word2vec_train_data = list(map(lambda x: x.split(), X_train))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g111XOZuYWEZ"},"outputs":[],"source":["#Defining the model and training it.\n","word2vec_model = Word2Vec(Word2vec_train_data,\n","                 vector_size=Embedding_dimensions,\n","                 workers=8,\n","                 min_count=5)\n","\n","print(\"Vocabulary Length:\", len(word2vec_model.wv.key_to_index))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2WKiLVOWYWEa"},"outputs":[],"source":["# Defining the model input length.\n","input_length = 60\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-416ltNEYWEb"},"outputs":[],"source":["vocab_length = 60000\n","\n","tokenizer = Tokenizer(filters=\"\", lower=False, oov_token=\"<oov>\")\n","tokenizer.fit_on_texts(X_data)\n","tokenizer.num_words = vocab_length\n","print(\"Tokenizer vocab length:\", vocab_length)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EJGwJMwvYWEc"},"outputs":[],"source":["X_train = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=input_length)\n","X_test  = pad_sequences(tokenizer.texts_to_sequences(X_test) , maxlen=input_length)\n","\n","print(\"X_train.shape:\", X_train.shape)\n","print(\"X_test.shape :\", X_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QbxncbQMYWEd"},"outputs":[],"source":["embedding_matrix = np.zeros((vocab_length, Embedding_dimensions))\n","\n","for word, token in tokenizer.word_index.items():\n","    if word2vec_model.wv.__contains__(word):\n","        embedding_matrix[token] = word2vec_model.wv.__getitem__(word)\n","\n","print(\"Embedding Matrix Shape:\", embedding_matrix.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"to0ILNO_YWEd"},"outputs":[],"source":["from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, Dense, LSTM, Conv1D, Embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fD_ivS-3YWEe"},"outputs":[],"source":["def getModel():\n","    embedding_layer = Embedding(input_dim = vocab_length,\n","                                output_dim = Embedding_dimensions,\n","                                weights=[embedding_matrix],\n","                                input_length=input_length,\n","                                trainable=False)\n","\n","    model = Sequential([\n","        embedding_layer,\n","        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),\n","        Bidirectional(LSTM(100, dropout=0.3, return_sequences=True)),\n","        Conv1D(100, 5, activation='relu'),\n","        GlobalMaxPool1D(),\n","        Dense(16, activation='relu'),\n","        Dense(1, activation='sigmoid'),\n","    ],\n","    name=\"Sentiment_Model\")\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4DbAT3KoYWEf"},"outputs":[],"source":["training_model = getModel()\n","training_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k9NPhFW0YWEf"},"outputs":[],"source":["from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n","\n","callbacks = [ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n","             EarlyStopping(monitor='val_accuracy', min_delta=1e-4, patience=5)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xDf3JIOkYWEg"},"outputs":[],"source":["training_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5UoHiUIEYWEg"},"outputs":[],"source":["history = training_model.fit(\n","    X_train, y_train,\n","    batch_size=1024,\n","    epochs=12,\n","    validation_split=0.1,\n","    callbacks=callbacks,\n","    verbose=1,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3IrgTiH8YWEh"},"outputs":[],"source":["acc,  val_acc  = history.history['accuracy'], history.history['val_accuracy']\n","loss, val_loss = history.history['loss'], history.history['val_loss']\n","epochs = range(len(acc))\n","\n","plt.plot(epochs, acc, 'b', label='Training acc')\n","plt.plot(epochs, val_acc, 'r', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","\n","plt.figure()\n","\n","plt.plot(epochs, loss, 'b', label='Training loss')\n","plt.plot(epochs, val_loss, 'r', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"27YBUMYbYWEi"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, classification_report\n","\n","def ConfusionMatrix(y_pred, y_test):\n","    # Compute and plot the Confusion matrix\n","    cf_matrix = confusion_matrix(y_test, y_pred)\n","\n","    categories  = ['Negative','Positive']\n","    group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n","    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n","\n","    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n","    labels = np.asarray(labels).reshape(2,2)\n","\n","    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n","                xticklabels = categories, yticklabels = categories)\n","\n","    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n","    plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n","    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vZ6cCm5oYWEi"},"outputs":[],"source":["# Predicting on the Test dataset.\n","y_pred = training_model.predict(X_test)\n","\n","# Converting prediction to reflect the sentiment predicted.\n","y_pred = np.where(y_pred>=0.5, 1, 0)\n","\n","# Printing out the Evaluation metrics. \n","ConfusionMatrix(y_pred, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M86d-TGNYWEj"},"outputs":[],"source":["# Print the evaluation metrics for the dataset.\n","print(classification_report(y_test, y_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kp4FgkQBYWEj"},"outputs":[],"source":["# Saving Word2Vec-Model\n","word2vec_model.wv.save('Word2Vec')\n","word2vec_model.wv.save_word2vec_format('Word2Vec-100')\n","\n","# Saving the tokenizer\n","with open('Tokenizer.pickle', 'wb') as file:\n","    pickle.dump(tokenizer, file)\n","\n","# Saving the TF-Model.\n","training_model.save('Sentiment-BiLSTM')\n","training_model.save_weights(\"Model Weights/weights\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.2 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"598a2f3b50c1c2dda71daf7aff18104181475cf9b132f21d46967a184a024fbc"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}